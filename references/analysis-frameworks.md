# Analytical Frameworks for Risk Assessment

Use these mental models when analyzing subjects for the `black-swan-seeker` skill.

## 1. Taxonomy of Extreme Events

### ü¶¢ Black Swan (Taleb)
*   **Definition:** An event that is:
    1.  **Outlier:** Beyond the realm of regular expectations (unknown unknown).
    2.  **High Impact:** Carries extreme consequences.
    3.  **Retrospective Predictability:** Humans explain it afterwards as if it were predictable.
*   **Examples:** The Internet, 9/11, WWI, The 2008 Financial Crisis (for many).
*   **Detection Strategy:** Look for systems with "fat tails" (Scalable distributions) rather than Gaussian (Bell Curve) distributions.

### ü¶è Grey Rhino (Wucker)
*   **Definition:** A highly probable, high impact yet neglected threat. It is not random; it is coming, but we ignore it.
*   **Examples:** Climate change impact on real estate, aging demographics, rising national debt.
*   **Detection Strategy:** Look for warnings from experts that are being dismissed by the mainstream or markets.

### üêâ Dragon King (Sornette)
*   **Definition:** An extreme event generated by the specific mechanisms of the system itself (endogenous), often predictable if you understand the system's dynamics (e.g., positive feedback loops).
*   **Examples:** Financial bubbles bursting, epilepsy seizures.
*   **Detection Strategy:** Look for "super-exponential growth" (unsustainable booms) and phase transitions.

## 2. Structural Analysis Models

### The Turkey Problem
*   **Concept:** A turkey is fed every day for 1,000 days. Its confidence that "the farmer loves me" increases every day. On day 1,001 (Thanksgiving), the surprise occurs.
*   **Application:** Past stability is not evidence of future safety. In fact, prolonged stability can breed instability (Minsky Moment).

### Via Negativa (Subtraction)
*   **Concept:** We know more about what is wrong than what is right.
*   **Application:** Instead of looking for what makes a system "good", look for what makes it "fragile". Removing a fragility is more robust than adding a feature.

### Iatrogenics (Harm from the Healer)
*   **Concept:** Damage caused by the treatment itself.
*   **Application:** In tech/policy, look for "interventions" that create new, hidden risks (e.g., AI moderation creating bias, infinite liquidity creating asset bubbles).

### The Lindy Effect
*   **Concept:** For non-perishable things (ideas, technologies), the expected remaining life is proportional to its current age. Old things survive.
*   **Application:** New, unproven technologies or methods are statistically more fragile than established ones. "New paradigm" is a red flag.

## 3. Systems Theory

### Tight Coupling
*   **Definition:** Parts of the system are rigidly connected; there is no slack or buffer.
*   **Risk:** A failure in one node propagates instantly to others.

### Complex vs. Complicated
*   **Complicated:** A car engine. Many parts, but predictable relationships. Solvable.
*   **Complex:** Traffic. Many agents interacting, emergent behavior. Unpredictable.
*   **Risk:** Treating complex systems (markets, ecology, AI) as merely complicated ones leads to disaster.
